{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NX9tfCuKw_xg"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain==1.0.5 langchain-community==0.4.1 langchain_google_genai==3.0.1 langchain-core==1.0.4 faiss-cpu==1.12.0 python-dotenv==1.2.1 pypdf==6.2.0 serpapi==0.1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2t-KVrmNEqG",
        "outputId": "75bd19be-bf9e-445d-bb69-e8eb9b2096b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 1.0.5\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://docs.langchain.com/\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langgraph, pydantic\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-community\n",
            "Version: 0.4.1\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, httpx-sse, langchain-classic, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-google-genai\n",
            "Version: 3.0.1\n",
            "Summary: An integration package connecting Google's genai package and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filetype, google-ai-generativelanguage, langchain-core, pydantic\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-core\n",
            "Version: 1.0.4\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://docs.langchain.com/\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: jsonpatch, langsmith, packaging, pydantic, pyyaml, tenacity, typing-extensions\n",
            "Required-by: langchain, langchain-classic, langchain-community, langchain-google-genai, langchain-text-splitters, langgraph, langgraph-checkpoint, langgraph-prebuilt\n",
            "---\n",
            "Name: faiss-cpu\n",
            "Version: 1.12.0\n",
            "Summary: A library for efficient similarity search and clustering of dense vectors.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Kota Yamaguchi <yamaguchi_kota@cyberagent.co.jp>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: numpy, packaging\n",
            "Required-by: \n",
            "---\n",
            "Name: python-dotenv\n",
            "Version: 1.2.1\n",
            "Summary: Read key-value pairs from a .env file and set them as environment variables\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Saurabh Kumar <me+github@saurabh-kumar.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: \n",
            "Required-by: google-adk, pydantic-settings\n",
            "---\n",
            "Name: pypdf\n",
            "Version: 6.2.0\n",
            "Summary: A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Mathieu Fenniak <biziqe@mathieu.fenniak.net>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "---\n",
            "Name: serpapi\n",
            "Version: 0.1.5\n",
            "Summary: The official Python client for SerpApi.com.\n",
            "Home-page: https://github.com/serpapi/serpapi-python\n",
            "Author: SerpApi.com\n",
            "Author-email: kenneth@serpapi.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: requests\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show langchain langchain-community langchain_google_genai langchain-core faiss-cpu python-dotenv pypdf serpapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FfwLZMWwqO8"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "# from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.messages import AIMessage,ToolMessage\n",
        "import json\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWHjqroExz5K"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "# Load Gemini API key from Colab secrets\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
        "SERPAPI_API_KEY=userdata.get('SERPAPI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGFiILUQxQNk"
      },
      "outputs": [],
      "source": [
        "messages = []\n",
        "\n",
        "# ‚úÖ Load API key\n",
        "# load_dotenv()\n",
        "# SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\") # Removed\n",
        "if not SERPAPI_API_KEY: # Changed to use the variable loaded from Colab secrets\n",
        "    raise ValueError(\"‚ùå SERPAPI_API_KEY not found. Please check your .env file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YE7kviSyzgo"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Load and process PDF\n",
        "pdf_path = \"/content/scholarship_info.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW_WIGDz0HNJ"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Set up LLM and RAG chain\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krUyRled0SVm"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Define tools using modern `@tool` decorator\n",
        "@tool\n",
        "def pdf_scholarship_search(query: str) -> str:\n",
        "    \"\"\"Answer scholarship-related questions using the loaded PDF.\"\"\"\n",
        "    # Retrieve relevant docs\n",
        "    return retriever.invoke(query)\n",
        "\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Search the web using SerpAPI for up-to-date information.\"\"\"\n",
        "    try:\n",
        "        search = SerpAPIWrapper()\n",
        "        return search.run(query)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Failed to search the web: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd3bV2IN0TIY"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools([pdf_scholarship_search, web_search])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ney64kuL0W8Z",
        "outputId": "801e4ded-cf1e-424c-9888-7083cf7a4b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéì Welcome to the Scholarship Assistant!\n",
            "Type your question below (or type 'exit' to quit):\n",
            "\n",
            "üß† Your question: Who is this scholarship for?\n",
            "\n",
            "üìå Answer:\n",
            "This scholarship is for students in India who are pursuing undergraduate degrees. They must have an annual family income below ‚Çπ6,00,000 and a minimum of 60% marks in their last qualifying exam.\n",
            "\n",
            "üß† Your question: What are the benefits of the scholarship?\n",
            "\n",
            "üìå Answer:\n",
            "The scholarship offers ‚Çπ10,000 per semester for tuition and a book allowance of ‚Çπ3,000 per year.\n",
            "\n",
            "üß† Your question: exit\n",
            "üëã Exiting. Good luck with your scholarships!\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Interactive loop\n",
        "print(\"\\nüéì Welcome to the Scholarship Assistant!\")\n",
        "print(\"Type your question below (or type 'exit' to quit):\")\n",
        "\n",
        "messages = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nüß† Your question: \").strip()\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Exiting. Good luck with your scholarships!\")\n",
        "        break\n",
        "\n",
        "    # 1. Add user input\n",
        "    messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    # 2. LLM decides what to do\n",
        "    ai_message = llm_with_tools.invoke(messages)\n",
        "    messages.append(ai_message)\n",
        "\n",
        "    # 3. If tool call, run tool and append result\n",
        "    if ai_message.tool_calls:\n",
        "        for call in ai_message.tool_calls:\n",
        "            tool_name = call[\"name\"]\n",
        "            tool_args = call[\"args\"]\n",
        "\n",
        "            if tool_name == \"web_search\":\n",
        "                tool_output = web_search.invoke(tool_args)\n",
        "            elif tool_name == \"pdf_scholarship_search\":\n",
        "                tool_output = pdf_scholarship_search.invoke(tool_args)\n",
        "            else:\n",
        "                tool_output = f\"Unknown tool: {tool_name}\"\n",
        "\n",
        "            # 4. Append tool result as a message\n",
        "            messages.append(ToolMessage(tool_call_id=call[\"id\"], content=tool_output))\n",
        "\n",
        "        # 5. Ask LLM to give final answer using tool result\n",
        "        final_response = llm_with_tools.invoke(messages)\n",
        "        print(\"\\nüìå Answer:\")\n",
        "        print(final_response.content)\n",
        "        messages.append(final_response)\n",
        "\n",
        "    else:\n",
        "        # No tool needed ‚Äî just print answer\n",
        "        print(\"\\nüìå Answer:\")\n",
        "        print(ai_message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsHYs3dkDa3F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}